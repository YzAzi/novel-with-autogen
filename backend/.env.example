# Backend runtime config (copy to repo root as .env, or pass env vars in Docker)

DB_PATH=/data/app.db
BACKEND_CORS_ORIGINS=http://localhost:3000

# LLM: keep MOCK_LLM=1 to run without any network/model.
MOCK_LLM=1
LLM_API_KEY=
LLM_BASE_URL=
LLM_MODEL=gpt-4o-mini
LLM_TEMPERATURE=0.7

# RAG: ChromaDB local persistence (default path is relative to backend/).
CHROMA_PERSIST_DIR=data/chroma

# Embeddings / reranker (local models). If model load fails, auto-fallback to mock.
EMBEDDINGS_PROVIDER=local_bge_m3    # local_bge_m3|mock
# HuggingFace 模型名或本地路径（离线：model/bge-m3；Docker 挂载为 /models/bge-m3）
BGE_M3_MODEL_NAME=model/bge-m3
RERANK_PROVIDER=local_bge           # local_bge|mock
# 离线：model/bge-reranker-v2-m3；Docker 挂载为 /models/bge-reranker-v2-m3
BGE_RERANK_MODEL_NAME=model/bge-reranker-v2-m3
RAG_DEVICE=cpu                      # cpu|cuda (optional)

# 强制离线（可选）
HF_HUB_OFFLINE=1
TRANSFORMERS_OFFLINE=1

# Chunking & retrieval knobs
RAG_MAX_CHUNK_CHARS=1400
RAG_OVERLAP_RATIO=0.2
RAG_TOP_K_V=10
RAG_TOP_K_KW=10

# Consistency critic
CRITIC_PROVIDER=mock                # llm|mock
AUTO_REVISE=false                   # true -> allow revised_text as final chapter
